{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO93OcdEfOQHFyuShnlEO94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MU2324/AI/blob/main/AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6T-Sd6pSLRCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BFS\n",
        "from collections import deque\n",
        "def bfs(graph, start, destination):\n",
        "    queue = deque([(start, [start])])\n",
        "    visited = set()\n",
        "    while queue:\n",
        "        current, path = queue.popleft()\n",
        "        if current == destination:\n",
        "            print(\"Path found:\", ' -> '.join(path))\n",
        "            return\n",
        "        visited.add(current)\n",
        "        for neighbor in graph[current]:\n",
        "            if neighbor not in visited:\n",
        "                queue.append((neighbor, path + [neighbor]))\n",
        "                visited.add(neighbor)\n",
        "india_city_graph = {\n",
        "    'Delhi': ['Mumbai', 'Jaipur', 'Lucknow'],\n",
        "    'Mumbai': ['Delhi', 'Chennai', 'Bangalore'],\n",
        "    'Chennai': ['Mumbai', 'Bangalore'],\n",
        "    'Bangalore': ['Mumbai', 'Chennai', 'Hyderabad'],\n",
        "    'Hyderabad': ['Bangalore', 'Chennai'],\n",
        "    'Jaipur': ['Delhi', 'Lucknow'],\n",
        "    'Lucknow': ['Delhi', 'Jaipur']\n",
        "}\n",
        "start_city = input(\"Enter the start city in India: \")\n",
        "destination_city = input(\"Enter the destination city in India: \")\n",
        "print(f\"Searching for a path from {start_city} to {destination_city}\")\n",
        "bfs(india_city_graph, start_city, destination_city)\n"
      ],
      "metadata": {
        "id": "CjhkYwebcbD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DFS\n",
        "\n",
        "def dfs(graph, current, destination, visited=None, path=None):\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path = path + [current]\n",
        "    visited.add(current)\n",
        "    if current == destination:\n",
        "        print(\"Path found:\", ' -> '.join(path))\n",
        "        return\n",
        "    for neighbor in graph[current]:\n",
        "        if neighbor not in visited:\n",
        "            dfs(graph, neighbor, destination, visited, path)\n",
        "india_city_graph = {\n",
        "    'Delhi': ['Mumbai', 'Jaipur', 'Lucknow'],\n",
        "    'Mumbai': ['Delhi', 'Chennai', 'Bangalore'],\n",
        "    'Chennai': ['Mumbai', 'Bangalore'],\n",
        "    'Bangalore': ['Mumbai', 'Chennai', 'Hyderabad'],\n",
        "    'Hyderabad': ['Bangalore', 'Chennai'],\n",
        "    'Jaipur': ['Delhi', 'Lucknow'],\n",
        "    'Lucknow': ['Delhi', 'Jaipur']\n",
        "}\n",
        "start_city = input(\"Enter the start city in India: \")\n",
        "destination_city = input(\"Enter the destination city in India: \")\n",
        "print(f\"Searching for a path from {start_city} to {destination_city}\")\n",
        "dfs(india_city_graph, start_city, destination_city)\n"
      ],
      "metadata": {
        "id": "tlQwQGfueeF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A* ALGORITHM\n",
        "\n",
        "import heapq\n",
        "# Define the graph of cities and their distances\n",
        "graph = {\n",
        "    'Mumbai': {'Pune': 150, 'Nashik': 170},\n",
        "    'Pune': {'Mumbai': 150,  'Sambhajinagar': 260},\n",
        "    'Nashik': {'Mumbai': 170, 'Sambhajinagar': 180},\n",
        "    'Sambhajinagar': {'Pune': 260, 'Nashik': 180}\n",
        "}\n",
        "# Heuristic function to estimate distance between two cities\n",
        "heuristic = {\n",
        "    'Mumbai': 0,\n",
        "    'Pune': 120,\n",
        "    'Nashik': 200,\n",
        "    'Sambhajinagar': 300\n",
        "}\n",
        "def astar(start, goal):\n",
        "    # Create a priority queue to store the nodes to be explored\n",
        "    open_list = [(0, start)]\n",
        "\n",
        "    # Create a set to store the visited nodes\n",
        "    closed_list = set()\n",
        "\n",
        "    # Create a dictionary to store the actual distance from start to each node\n",
        "    g = {city: float('inf') for city in graph}\n",
        "    g[start] = 0\n",
        "\n",
        "    # Create a dictionary to store the estimated total distance from start to goal via each node\n",
        "    f = {city: float('inf') for city in graph}\n",
        "    f[start] = heuristic[start]\n",
        "\n",
        "    # Create a dictionary to store the path taken to reach each node\n",
        "    path = {start: []}\n",
        "    while open_list:\n",
        "        # Get the node with the lowest total estimated distance\n",
        "        current_distance, current_city = heapq.heappop(open_list)\n",
        "\n",
        "        # Check if the goal is reached\n",
        "        if current_city == goal:\n",
        "            return g[current_city], path[current_city]\n",
        "\n",
        "        # Add the current city to the closed list\n",
        "        closed_list.add(current_city)\n",
        "\n",
        "        # Explore the neighbors of the current city\n",
        "        for neighbor, distance in graph[current_city].items():\n",
        "            # Calculate the actual distance from start to the neighbor\n",
        "            temp_g = g[current_city] + distance\n",
        "\n",
        "            # Check if the neighbor has not been visited or a shorter path is found\n",
        "            if neighbor not in closed_list and temp_g < g[neighbor]:\n",
        "                # Update the actual distance\n",
        "                g[neighbor] = temp_g\n",
        "                # Update the estimated total distance\n",
        "                f[neighbor] = temp_g + heuristic[neighbor]\n",
        "                # Add the neighbor to the open list\n",
        "                heapq.heappush(open_list, (f[neighbor], neighbor))\n",
        "                # Update the path taken to reach the neighbor\n",
        "                path[neighbor] = path[current_city] + [(current_city, neighbor)]\n",
        "    # No path found\n",
        "    return None, None\n",
        "\n",
        "# Print available cities\n",
        "print(\"Available cities:\")\n",
        "for city in graph.keys():\n",
        "    print(city)\n",
        "# Take user input for start and goal cities\n",
        "start_city = input(\"Enter the start city: \")\n",
        "goal_city = input(\"Enter the goal city: \")\n",
        "\n",
        "# Find the shortest path between the user-provided cities\n",
        "shortest_distance, shortest_path = astar(start_city, goal_city)\n",
        "\n",
        "if shortest_distance is not None:\n",
        "    print(f\"The shortest distance between {start_city} and {goal_city} is {shortest_distance} km.\")\n",
        "    print(\"The path is:\")\n",
        "    for city1, city2 in shortest_path:\n",
        "        print(f\"{city1} -> {city2}\")\n",
        "else:\n",
        "    print(f\"No path found between {start_city} and {goal_city}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EP3faG4lev24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "model = clf.fit(X_train , y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test , y_pred)\n",
        "print(\"Accuracy is \",accuracy)\n",
        "\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "image = tree.plot_tree(clf, feature_names=iris.feature_names, class_names=class_names, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U6DTlEMce70y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset as an example\n",
        "iris = datasets.load_digits()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(X)\n",
        "print(\"========================\")\n",
        "print(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "cls = svm.SVC(kernel=\"linear\")\n",
        "X_train,y_train =iris.data[:-10],iris.target[:-10]\n",
        "\n",
        "# Train the classifier on the training data\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = cls.predict(X_test)\n",
        "\n",
        "print(cls.predict(iris.data[:-10]))\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "plt.imshow(iris.images[9], interpolation='nearest')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XuJHgT_1jYlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=42)\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X_train , y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test , y_pred)\n",
        "print('accuracy ',accuracy)"
      ],
      "metadata": {
        "id": "1we7usxwoiUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FEED FORWARD BACK PRAPOGATION\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y_binary , test_size = 0.2)\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(8,input_dim = X_train.shape[1],activation = 'relu'),\n",
        "                             tf.keras.layers.Dense(1,activation='sigmoid')])\n",
        "\n",
        "model.compile(optimizer = 'adam' , loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train , y_train , epochs=50 , batch_size=32 , validation_split=0.2 , verbose = 0)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_test , y_pred_classes)\n",
        "print('accuracy is ',accuracy)"
      ],
      "metadata": {
        "id": "c7QupJ1npYrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAIVE BAYES\n",
        "\n",
        "# load the iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# store the feature matrix (X) and response vector (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# splitting X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "# training the model on training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# making predictions on the testing set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
        "from sklearn import metrics\n",
        "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YHYu3PDZpfdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adaboost\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=1234)\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=1)\n",
        "clf.fit(X_train , y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test , y_pred)\n",
        "print('Accuracy of individual tree ',accuracy)\n",
        "\n",
        "boost_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3) , n_estimators=50 , random_state=1234)\n",
        "boost_clf.fit(X_train , y_train)\n",
        "\n",
        "y_adaboost_pred = boost_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test , y_adaboost_pred)\n",
        "print('Accuracy after boosting ',accuracy)"
      ],
      "metadata": {
        "id": "tUKYWwoLy13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Association Rule Mining\n",
        "\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori , association_rules\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "transactions = [\n",
        "    ['orange', 'lemon', 'banana'],\n",
        "    ['orange', 'coconut'],\n",
        "    ['lemon', 'coconut'],\n",
        "    ['orange', 'lemon', 'coconut'],\n",
        "    ['orange', 'banana'],\n",
        "]\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_array = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_array , columns = te.columns_)\n",
        "\n",
        "frequent_items = apriori(df , min_support=0.2 , use_colnames=True)\n",
        "\n",
        "association_rules =  association_rules(frequent_items , metric='confidence' , min_threshold=0.5)\n",
        "\n",
        "print(\"Frequent Itemsets:\")\n",
        "print(frequent_items)\n",
        "print(\"\\n\\nAssociation Rules:\")\n",
        "print(association_rules)\n",
        "\n",
        "plt.barh(range(len(frequent_items)) , frequent_items['support'] , align='center')\n",
        "plt.yticks(range(len(frequent_items)) , frequent_items['itemsets'].apply(lambda x:','.join(x)))\n",
        "plt.xlabel('Support')\n",
        "plt.title('Frequent Items')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BoZXXr6jJNxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}